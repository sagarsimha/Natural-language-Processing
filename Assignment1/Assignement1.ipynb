{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Summer 2020 - Assignement 1 - Exercise 4 \n",
    "## Vishaal Khanna - 120333\n",
    "## Sneha Mohanty - 120799\n",
    "## Sagar Nagaraj Simha - 120797"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 a) Recalculation of statistical parameters from Exercise 3 for austen, bronte and disputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for file: austen.txt:\n",
      "number of tokens: 122880\n",
      "vocabulary size: 6322\n",
      "type-token-ratio: 0.05144856770833333\n",
      "mean token length: 4.3663330078125\n",
      "entropy: 9.066683301108936\n",
      "5 most frequent 1-grams: [(('the',), 4331), (('to',), 4163), (('of',), 3611), (('and',), 3585), (('her',), 2225)]\n",
      "5 most frequent 2-grams: [(('of', 'the'), 464), (('to', 'be'), 443), (('in', 'the'), 382), (('i', 'am'), 303), (('mr', 'darcy'), 273)]\n",
      "5 most frequent 3-grams: [(('i', 'do', 'not'), 62), (('i', 'am', 'sure'), 62), (('as', 'soon', 'as'), 55), (('she', 'could', 'not'), 50), (('that', 'he', 'had'), 37)]\n",
      "3 most frequent 4-grams: [(('i', 'do', 'not', 'know'), 19), (('at', 'the', 'same', 'time'), 16), (('the', 'rest', 'of', 'the'), 15)]\n",
      "3 most frequent 5-grams: [(('as', 'soon', 'as', 'they', 'were'), 9), (('mrs', 'hurst', 'and', 'miss', 'bingley'), 7), (('in', 'the', 'course', 'of', 'the'), 7)]\n",
      "\n",
      "\n",
      "\n",
      "statistics for file: bronte.txt:\n",
      "number of tokens: 188459\n",
      "vocabulary size: 12484\n",
      "type-token-ratio: 0.06624252489931497\n",
      "mean token length: 4.160560121830213\n",
      "entropy: 9.562135766603287\n",
      "5 most frequent 1-grams: [(('the',), 7806), (('i',), 7245), (('and',), 6606), (('to',), 5190), (('a',), 4460)]\n",
      "5 most frequent 2-grams: [(('of', 'the'), 807), (('in', 'the'), 610), (('i', 'had'), 510), (('i', 'was'), 482), (('to', 'the'), 433)]\n",
      "5 most frequent 3-grams: [(('i', 'could', 'not'), 109), (('i', 'did', 'not'), 60), (('it', 'was', 'a'), 57), (('i', 'don', 't'), 51), (('i', 'am', 'not'), 46)]\n",
      "3 most frequent 4-grams: [(('in', 'the', 'course', 'of'), 12), (('i', 'don', 't', 'know'), 11), (('in', 'the', 'midst', 'of'), 10)]\n",
      "3 most frequent 5-grams: [(('as', 'well', 'as', 'i', 'could'), 6), (('at', 'the', 'foot', 'of', 'the'), 5), (('in', 'the', 'course', 'of', 'the'), 5)]\n",
      "\n",
      "\n",
      "\n",
      "statistics for file: disputed.txt:\n",
      "number of tokens: 161975\n",
      "vocabulary size: 7099\n",
      "type-token-ratio: 0.04382775119617225\n",
      "mean token length: 4.22497916345115\n",
      "entropy: 9.019762786285101\n",
      "5 most frequent 1-grams: [(('to',), 5242), (('the',), 5204), (('and',), 4897), (('of',), 4293), (('i',), 3192)]\n",
      "5 most frequent 2-grams: [(('to', 'be'), 608), (('of', 'the'), 566), (('it', 'was'), 449), (('in', 'the'), 446), (('i', 'am'), 395)]\n",
      "5 most frequent 3-grams: [(('i', 'do', 'not'), 136), (('i', 'am', 'sure'), 109), (('she', 'could', 'not'), 73), (('a', 'great', 'deal'), 64), (('it', 'would', 'be'), 63)]\n",
      "3 most frequent 4-grams: [(('i', 'do', 'not', 'know'), 50), (('a', 'great', 'deal', 'of'), 26), (('i', 'am', 'sure', 'i'), 20)]\n",
      "3 most frequent 5-grams: [(('i', 'have', 'no', 'doubt', 'of'), 10), (('a', 'quarter', 'of', 'an', 'hour'), 8), (('i', 'do', 'not', 'know', 'what'), 7)]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import string\n",
    "from math import log2\n",
    "\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "AUSTEN = 'austen.txt'\n",
    "BRONTE = 'bronte.txt'\n",
    "DISPUTED = 'disputed.txt'\n",
    "\n",
    "special_chars = string.punctuation + \"``--''''\"\n",
    "\n",
    "\n",
    "def compute_stats(filename):\n",
    "    token_freq_dict = dict()\n",
    "    mean_token_length = 0.0\n",
    "    token_list = list()\n",
    "\n",
    "    with open(filename) as f:\n",
    "        text = f.read()\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        for token in tokenizer.tokenize(text):\n",
    "        #for token in nltk.word_tokenize(text):\n",
    "            #if token not in special_chars:\n",
    "                token = token.lower()\n",
    "                current_count = token_freq_dict.get(token, 0)\n",
    "                token_freq_dict[token] = current_count + 1\n",
    "                mean_token_length += len(token)\n",
    "                token_list.append(token)\n",
    "\n",
    "    num_tokens = sum([frequency for token, frequency in token_freq_dict.items()])\n",
    "    vocabulary_size = len(token_freq_dict)    # number of unique tokens\n",
    "    type_token_ratio = float(vocabulary_size / num_tokens)    # number of unique tokens / total number of tokens\n",
    "    mean_token_length /= num_tokens    # total length of all tokens / total number of tokens\n",
    "\n",
    "    # computing entropy\n",
    "    entropy = 0.0\n",
    "    for frequency in token_freq_dict.values():\n",
    "        token_probability = float(frequency / num_tokens)\n",
    "        token_entropy = token_probability * log2(token_probability)    # calculate entropy for every token and add to total entropy\n",
    "        entropy += token_entropy\n",
    "\n",
    "    entropy = entropy * (-1)\n",
    "\n",
    "    # computing most frequent ngrams, reference - https://www.kaggle.com/rtatman/tutorial-getting-n-grams\n",
    "    unigrams = ngrams(token_list, 1)\n",
    "    bigrams = ngrams(token_list, 2)\n",
    "    trigrams = ngrams(token_list, 3)\n",
    "    fourgrams = ngrams(token_list, 4)\n",
    "    fivegrams = ngrams(token_list, 5)\n",
    "\n",
    "    unigram_freq = collections.Counter(unigrams)\n",
    "    bigram_freq = collections.Counter(bigrams)\n",
    "    trigram_freq = collections.Counter(trigrams)\n",
    "    fourgrams_freq = collections.Counter(fourgrams)\n",
    "    fivegrams_freq = collections.Counter(fivegrams)\n",
    "\n",
    "    top_unigrams = unigram_freq.most_common(5)\n",
    "    top_bigrams = bigram_freq.most_common(5)\n",
    "    top_trigrams = trigram_freq.most_common(5)\n",
    "    top_fourgrams = fourgrams_freq.most_common(3)\n",
    "    top_fivegrams = fivegrams_freq.most_common(3)\n",
    "\n",
    "    print('statistics for file: {}:'.format(filename))\n",
    "    print('number of tokens: {}'.format(num_tokens))\n",
    "    print('vocabulary size: {}'.format(vocabulary_size))\n",
    "    print('type-token-ratio: {}'.format(type_token_ratio))\n",
    "    print('mean token length: {}'.format(mean_token_length))\n",
    "    print('entropy: {}'.format(entropy))\n",
    "    print('5 most frequent 1-grams: {}'.format(top_unigrams))\n",
    "    print('5 most frequent 2-grams: {}'.format(top_bigrams))\n",
    "    print('5 most frequent 3-grams: {}'.format(top_trigrams))\n",
    "    print('3 most frequent 4-grams: {}'.format(top_fourgrams))\n",
    "    print('3 most frequent 5-grams: {}'.format(top_fivegrams))\n",
    "    print('\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    compute_stats(AUSTEN)\n",
    "    compute_stats(BRONTE)\n",
    "    compute_stats(DISPUTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference - Based on the following three observations, the most likely author of the disputed text is austen\n",
    "1. Vocabulary size is comparable (austen - 6322, disputed - 7099)\n",
    "2. Entropy is close - (austen - 9.06, disputed - 9.01)\n",
    "3. The frequent 3grams are similar between austen and the disputed author "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import math\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each of the documents from file\n",
    "with open('austen.txt') as f:\n",
    "        austen = f.read()\n",
    "with open('bronte.txt') as f:\n",
    "        bronte = f.read()\n",
    "with open('disputed.txt') as f:\n",
    "        disputed = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize(words) removing characters and punctuations\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_austen = tokenizer.tokenize(austen)\n",
    "tokenized_bronte = tokenizer.tokenize(bronte)\n",
    "tokenized_disputed = tokenizer.tokenize(disputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase the tokens \n",
    "tokenized_austen = [token.lower() for token in tokenized_austen]\n",
    "tokenized_bronte = [token.lower() for token in tokenized_bronte]\n",
    "tokenized_disputed = [token.lower() for token in tokenized_disputed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined document\n",
    "#tokenized_all_documents = tokenized_austen + tokenized_bronte + tokenized_disputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of document austen, bronte and disputed respectively are: 122880 188459 161975\n"
     ]
    }
   ],
   "source": [
    "# Computing the length of individual documents\n",
    "len_austen = len(tokenized_austen)\n",
    "len_bronte = len(tokenized_bronte)\n",
    "len_disputed = len(tokenized_disputed)\n",
    "print('Length of document austen, bronte and disputed respectively are:',len_austen,len_bronte,len_disputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_vocabulary across all documents\n",
    "common_vocabulary = list(set(tokenized_austen) & set(tokenized_bronte) & set(tokenized_disputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of common vocubaulary among the three documents 3658\n"
     ]
    }
   ],
   "source": [
    "N = len(common_vocabulary)\n",
    "print('Length of common vocubaulary among the three documents', N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean relative frequency of each word from common vocabulary (of size N) in all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean relative frequency of the word i over all documents\n",
    "mean_vocab_words = []\n",
    "for word in common_vocabulary:\n",
    "    mean_word = ((tokenized_austen.count(word)/len_austen) + (tokenized_bronte.count(word)/len_bronte) + (tokenized_disputed.count(word)/len_disputed))/ 3.0\n",
    "    mean_vocab_words.append(mean_word)\n",
    "\n",
    "#Total_documents_length = len(tokenized_all_documents)\n",
    "#Mean relative frequency of the word i in all documents\n",
    "#mean_vocab_words = []\n",
    "#for word in common_vocabulary:\n",
    "    #mean_vocab_words.append(tokenized_all_documents.count(word)/Total_documents_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative frequency of each word in the each of the 3 documents as a 2 D matrix of size Nx3 (fi(D), where i is [1,N] and D is [1,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intial empty dataframe with column names that will be filled up with each word in vocabulary\n",
    "relativefrequency_word = pd.DataFrame(columns=['austen','bronte','disputed'])\n",
    "#relativefrequency_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in common_vocabulary:\n",
    "    relativefrequency_word = relativefrequency_word.append({'austen' : (tokenized_austen.count(word)/len_austen) , 'bronte' : (tokenized_bronte.count(word)/len_bronte), 'disputed' : (tokenized_disputed.count(word)/len_disputed)} , ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deviation sigma of relative frequency of each word over the 3 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard deviation of the relative frequency of the word i over all documents - https://en.wikipedia.org/wiki/Standard_deviation\n",
    "sigma_ofRF_eachword = []\n",
    "for i in range(len(common_vocabulary)):\n",
    "    sigma_ofRF_eachword.append(math.sqrt((pow(relativefrequency_word.iloc[i]['austen'] - mean_vocab_words[i], 2) + pow(relativefrequency_word.iloc[i]['bronte'] - mean_vocab_words[i], 2) + pow(relativefrequency_word.iloc[i]['disputed'] - mean_vocab_words[i], 2))/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-scores for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intial empty dataframe with column names that will be filled up with each word in vocabulary\n",
    "Zscores = pd.DataFrame(columns=['word','austen','bronte','disputed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word in common vocabulary\n",
    "for i in range(len(common_vocabulary)):\n",
    "    zaus = (relativefrequency_word.iloc[i]['austen'] - mean_vocab_words[i]) / sigma_ofRF_eachword[i]\n",
    "    zbron = (relativefrequency_word.iloc[i]['bronte'] - mean_vocab_words[i]) / sigma_ofRF_eachword[i]\n",
    "    zdis = (relativefrequency_word.iloc[i]['disputed'] - mean_vocab_words[i]) / sigma_ofRF_eachword[i]\n",
    "    Zscores = Zscores.append({'word': common_vocabulary[i], 'austen' : zaus , 'bronte' : zbron, 'disputed' : zdis} , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>austen</th>\n",
       "      <th>bronte</th>\n",
       "      <th>disputed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>returning</td>\n",
       "      <td>0.572293</td>\n",
       "      <td>-1.406129</td>\n",
       "      <td>0.833836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>-0.326474</td>\n",
       "      <td>1.354900</td>\n",
       "      <td>-1.028426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seats</td>\n",
       "      <td>-0.093636</td>\n",
       "      <td>1.268875</td>\n",
       "      <td>-1.175239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>regarding</td>\n",
       "      <td>-0.460749</td>\n",
       "      <td>1.388296</td>\n",
       "      <td>-0.927547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sincerely</td>\n",
       "      <td>1.413876</td>\n",
       "      <td>-0.733684</td>\n",
       "      <td>-0.680192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3653</th>\n",
       "      <td>connected</td>\n",
       "      <td>1.350353</td>\n",
       "      <td>-1.039058</td>\n",
       "      <td>-0.311295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3654</th>\n",
       "      <td>law</td>\n",
       "      <td>1.193673</td>\n",
       "      <td>-1.253616</td>\n",
       "      <td>0.059943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>spot</td>\n",
       "      <td>0.054950</td>\n",
       "      <td>1.196345</td>\n",
       "      <td>-1.251295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>qualities</td>\n",
       "      <td>1.256984</td>\n",
       "      <td>-0.067249</td>\n",
       "      <td>-1.189735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>stood</td>\n",
       "      <td>-0.665961</td>\n",
       "      <td>1.413430</td>\n",
       "      <td>-0.747470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3658 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word    austen    bronte  disputed\n",
       "0     returning  0.572293 -1.406129  0.833836\n",
       "1          love -0.326474  1.354900 -1.028426\n",
       "2         seats -0.093636  1.268875 -1.175239\n",
       "3     regarding -0.460749  1.388296 -0.927547\n",
       "4     sincerely  1.413876 -0.733684 -0.680192\n",
       "...         ...       ...       ...       ...\n",
       "3653  connected  1.350353 -1.039058 -0.311295\n",
       "3654        law  1.193673 -1.253616  0.059943\n",
       "3655       spot  0.054950  1.196345 -1.251295\n",
       "3656  qualities  1.256984 -0.067249 -1.189735\n",
       "3657      stood -0.665961  1.413430 -0.747470\n",
       "\n",
       "[3658 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 most frequent words in austen, bronte and disputed\n",
    "austen_top50 = collections.Counter(tokenized_austen).most_common()[:50]\n",
    "austen_top50 = [i[0] for i in austen_top50]\n",
    "\n",
    "bronte_top50 = collections.Counter(tokenized_bronte).most_common()[:50]\n",
    "bronte_top50 = [i[0] for i in bronte_top50]\n",
    "\n",
    "disputed_top50 = collections.Counter(tokenized_disputed).most_common()[:50]\n",
    "disputed_top50 = [i[0] for i in disputed_top50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the Zscores of top50 words of each document \n",
    "austen_top50_zscores = Zscores[Zscores['word'].isin(austen_top50)]\n",
    "austen_top50_zscores = austen_top50_zscores[['word', 'austen']] \n",
    "\n",
    "bronte_top50_zscores = Zscores[Zscores['word'].isin(bronte_top50)]\n",
    "bronte_top50_zscores = bronte_top50_zscores[['word','bronte']]\n",
    "\n",
    "disputed_top50_zscores = Zscores[Zscores['word'].isin(disputed_top50)]\n",
    "disputed_top50_zscores = disputed_top50_zscores[['word','disputed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reordering: to reorder Zscores as in the order of top50 frequent words in pandas\n",
    "austen_top50_zscores['word_'] = pd.Categorical(\n",
    "    austen_top50_zscores['word'], \n",
    "    categories = austen_top50, \n",
    "    ordered = True\n",
    ")\n",
    "austen_top50_zscores = austen_top50_zscores.sort_values('word_')\n",
    "\n",
    "bronte_top50_zscores['word_'] = pd.Categorical(\n",
    "    bronte_top50_zscores['word'], \n",
    "    categories = bronte_top50, \n",
    "    ordered = True\n",
    ")\n",
    "bronte_top50_zscores = bronte_top50_zscores.sort_values('word_')\n",
    "\n",
    "disputed_top50_zscores['word_'] = pd.Categorical(\n",
    "    disputed_top50_zscores['word'], \n",
    "    categories = disputed_top50, \n",
    "    ordered = True\n",
    ")\n",
    "disputed_top50_zscores = disputed_top50_zscores.sort_values('word_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7feecd0cfed0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%matplotlib qt\n",
    "figure = plt.figure()\n",
    "subplot = figure.add_subplot()\n",
    "\n",
    "plt.ylabel('z scores', fontsize=16)\n",
    "\n",
    "index = 1\n",
    "width = 0\n",
    "\n",
    "for each in austen_top50_zscores['austen']:\n",
    "    if width==0:\n",
    "        plt.bar(index+width, each, 1, color='grey', label='austen')\n",
    "        width+=1\n",
    "    else:\n",
    "        plt.bar(index+width, each, 1, color='grey')\n",
    "        width+=1\n",
    "    \n",
    "index += 70\n",
    "\n",
    "width = 0\n",
    "for each in bronte_top50_zscores['bronte']:\n",
    "    if width==0:\n",
    "        plt.bar(index+width, each, 1, color='black', label='bronte')\n",
    "        width+=1\n",
    "    else:\n",
    "        plt.bar(index+width, each, 1, color='black')\n",
    "        width+=1\n",
    "    \n",
    "index += 70\n",
    "\n",
    "width = 0\n",
    "for each in disputed_top50_zscores['disputed']:\n",
    "    if width==0:\n",
    "        plt.bar(index+width, each, 1, color='blue', label='disputed')\n",
    "        width+=1\n",
    "    else:\n",
    "        plt.bar(index+width, each, 1, color='blue')\n",
    "        width+=1\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the bar chart of zscores of top 50 common words, the zscores for austen and disputed are more similar than that of bronte and disputed. The author is likely austen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta b for each pair each pair between the three documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6223.0405804149295"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltab_bronte_austen = sum((Zscores['bronte'] - Zscores['austen']).abs())\n",
    "deltab_bronte_austen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5962.898651827579"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltab_bronte_disputed = sum((Zscores['bronte'] - Zscores['disputed']).abs())\n",
    "deltab_bronte_disputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4859.135655462311"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltab_austen_disputed = sum((Zscores['austen'] - Zscores['disputed']).abs())\n",
    "deltab_austen_disputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower the Burrows Delta mean lower the Manhattan distance implying common use of words/phrases by the known author's and the disputed author. The texts from Austen and Disputed author have the least Burrows Delta of ~4859, while that between (Bronte, Austen) = ~6223, (Bronte, disputed author) = ~5962. Hence, the Author is most likely Austen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # References:\n",
    "### https://www.nltk.org/api/nltk.tokenize.html\n",
    "### https://en.wikipedia.org/wiki/Standard_deviation\n",
    "### https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781849513265/1/ch01lvl1sec16/plotting-multiple-bar-charts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
